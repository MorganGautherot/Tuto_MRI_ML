{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement convolutions in a whole net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data using pytorch MNIST function\n",
    "trainset = MNIST('../', download=True, train=True)\n",
    "testset = MNIST('../', download=True, train=False)\n",
    "\n",
    "# Store the labels\n",
    "y_trainset = trainset.targets\n",
    "y_testset = testset.targets\n",
    "\n",
    "# reshape and pass to float\n",
    "trainset = trainset.data.reshape(60000, -1).to(torch.float32)\n",
    "testset = testset.data.reshape(10000, -1).to(torch.float32)\n",
    "\n",
    "# Normalize\n",
    "m, std = trainset.mean(), trainset.std()\n",
    "trainset = (trainset - m) / std\n",
    "testset = (testset - m) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    # A convenience class to have all your data stored at the same place\n",
    "    # common practice\n",
    "    def __init__(self, x, y): self.x, self.y = x, y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    # helps us do minibatch training\n",
    "    def __init__(self, data, bs):\n",
    "        self.data, self.bs = data, bs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.data), self.bs):\n",
    "            yield self.data[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    # is used to get rid of code inside the training loop\n",
    "    def __init__(self, parameters, lr=0.4):\n",
    "        self.parameters, self.lr = list(parameters), lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                p -= p.grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return (torch.argmax(output, dim=1) == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 9, 6, 5, 4, 1, 7, 8, 5, 1, 1, 8, 8, 7, 4, 5, 2, 2, 5, 6, 5, 1, 4, 0,\n",
       "        5, 7, 1, 1, 9, 3, 6, 7, 4, 3, 4, 5, 8, 8, 5, 0, 7, 2, 1, 2, 8, 9, 2, 8,\n",
       "        1, 1, 7, 1, 6, 5, 4, 3, 4, 4, 8, 8, 9, 6, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explain accuracy\n",
    "my_output = torch.randn(64, 10)\n",
    "# my_output\n",
    "torch.argmax(my_output)  # only one number\n",
    "torch.argmax(my_output, dim=1)  # 64 numbers (one pred for each image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "lr = 0.3\n",
    "bs = 64\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "# here we will later download and assign pre-trained models\n",
    "learner = nn.Sequential(\n",
    "    nn.Linear(784, 250),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "\n",
    "opt = Optimizer(learner.parameters(), lr=lr)\n",
    "\n",
    "my_data = Dataset(trainset, y_trainset)\n",
    "train_dataloader = DataLoader(my_data, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0072, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    for xb, yb in train_dataloader:\n",
    "        out = learner(xb)\n",
    "        loss = loss_func(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 784])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = trainset[0:50000, :], trainset[50000:, :]\n",
    "y_train, y_valid = y_trainset[0:50000], y_trainset[50000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now start using Pytorch's DataLoader because it also has a random sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(Dataset(train, y_train), batch_size=64, shuffle=True)\n",
    "valid_dl = DataLoader(Dataset(valid, y_valid), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.1261846274137497 accuracy: 0.9681528806686401\n",
      "epoch 1 loss: 0.03386864438652992 accuracy: 0.9895501732826233\n",
      "epoch 2 loss: 0.03741159662604332 accuracy: 0.9880573153495789\n",
      "epoch 3 loss: 0.039337802678346634 accuracy: 0.9898487329483032\n",
      "epoch 4 loss: 0.03143204003572464 accuracy: 0.9913415312767029\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    learner.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(learner(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_acc = 0., 0.\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = learner(xb)\n",
    "            total_loss += loss_func(pred, yb)\n",
    "            total_acc += accuracy(pred, yb)\n",
    "        n_entries = len(valid_dl)\n",
    "        print('epoch', epoch,\n",
    "              'loss:', (total_loss/n_entries).item(),\n",
    "              'accuracy:', (total_acc/n_entries).item()\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adding convolutions is just a matter of changing the sequence of layers in nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Jupyter Notebook shortcts to access the doc and see how to use Conv2d\n",
    "# ??nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 784])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 9, 28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can basically replace the nn.Linear with this:\n",
    "my_layer = nn.Conv2d(in_channels=1, out_channels=9, kernel_size=3, padding=1)\n",
    "\n",
    "# PAY ATTENTION HERE\n",
    "# Except nn.Conv2d takes input of shape N * Channels * Height * Width\n",
    "# (as seen in the doc if you uncomment above)\n",
    "# and we had no channels so far (MNIST isn't RVB), so we add an extra channel\n",
    "my_layer(xb.reshape(16, 1, 28, 28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to flatten out the output of the successive convolutions\n",
    "# before we pass it to a nn.Linear()\n",
    "# we add a Lambda layer (pretty much like a lambda function) to do that\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return x.view(x.shape[0], -1)\n",
    "\n",
    "# we can also use that Lambda class to resize the data as in the above cell\n",
    "def mnist_resize(x): return x.view(-1, 1, 28, 28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 6\n",
    "lr = 0.4\n",
    "bs = 64\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "learner = nn.Sequential(\n",
    "    Lambda(mnist_resize),  # we can do the reshape here\n",
    "    nn.Conv2d(in_channels=1, out_channels=8,\n",
    "              kernel_size=3, stride=1, padding=1),  # bs*8*28*28\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 16, 3, 2, 1),  # bs*16*14*14\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 2, 1),  # bs * 32 * 7 * 7\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 2, 1),  # bs * 64 * 4 * 4\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, 2, 1),  # bs * 64 * 2 * 2\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(flatten),\n",
    "    nn.Linear(64, 10)\n",
    ")\n",
    "\n",
    "opt = Optimizer(learner.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.11320923268795013 accuracy: 0.9677547812461853\n",
      "epoch 1 loss: 0.07245137542486191 accuracy: 0.9804936051368713\n",
      "epoch 2 loss: 0.13093577325344086 accuracy: 0.959494411945343\n",
      "epoch 3 loss: 0.130466490983963 accuracy: 0.9604896306991577\n",
      "epoch 4 loss: 0.049570903182029724 accuracy: 0.9855692386627197\n",
      "epoch 5 loss: 0.0635807141661644 accuracy: 0.9829816818237305\n"
     ]
    }
   ],
   "source": [
    "# A cool thing is that the train loop doesn't need to change\n",
    "for epoch in range(EPOCHS):\n",
    "    learner.train()\n",
    "    for xb, yb in train_dl:\n",
    "        loss = loss_func(learner(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    learner.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_acc = 0., 0.\n",
    "        for xb, yb in valid_dl:\n",
    "            pred = learner(xb)\n",
    "            total_loss += loss_func(pred, yb)\n",
    "            total_acc += accuracy(pred, yb)\n",
    "        n_entries = len(valid_dl)\n",
    "        print('epoch', epoch,\n",
    "              'loss:', (total_loss/n_entries).item(),\n",
    "              'accuracy:', (total_acc/n_entries).item()\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we added convolutions, but two problems arose:\n",
    "\n",
    "* the training isn't very smooth. We'll need to add some regularization\n",
    "* the training is way slower than before. We'll need to start using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f04ff855910>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seven = testset.data[17].reshape(28, 28)\n",
    "cut_seven = seven[:, 5:22]\n",
    "shifted_seven = torch.zeros(28, 28)\n",
    "shifted_seven[:, :] = -0.4241\n",
    "shifted_seven[:, 0:17] = cut_seven\n",
    "plt.imshow(shifted_seven.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7]), tensor([7]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner(seven).argmax(dim=-1), learner(shifted_seven).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
